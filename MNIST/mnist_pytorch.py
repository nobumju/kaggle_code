# -*- coding: utf-8 -*-
"""Mnist_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C-y0frm3jmgW07RtoAK9iiNaqxduFcG8

데이터 불러오기
"""

!pip install kaggle
from google.colab import files
files.upload()

ls -1ha kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# Permission Warning 방지
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c digit-recognizer

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/digit-recognizer.zip

"""트레이닝"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader

class PATH:
    TRAIN = '/content/drive/MyDrive/Colab Notebooks/DATA/train.csv'
    TEST = '/content/drive/MyDrive/Colab Notebooks/DATA/test.csv'

class CONFIG:
    lr = 0.001
    epoch = 50
    batch_size = 256

df_train = pd.read_csv(PATH.TRAIN)
l = len(df_train)

if torch.cuda.is_available():
  device = 'cuda'
else :
  device = 'cpu'
print(device)

_x = torch.Tensor(df_train.iloc[:,1:].values).reshape(l,28,28) / 256 #28*28의 형태로 변환후 0~1 사이의 값으로 변환
_x = _x.unsqueeze(1) # (batch_size, depth, width, height)로 형태를 맞추기 위한 코드
_y = torch.Tensor(df_train.iloc[:,0].values).type(torch.long)

train_x ,valid_x = _x[:39000],_x[39000:] #train,valid 분활
train_y ,valid_y = _y[:39000],_y[39000:]

class DigitDataset(Dataset):

    def __init__(self,x,y=None):
        super(DigitDataset).__init__()
        self.x = x
        self.y = y

    def __getitem__(self,idx):
        if self.y == None:
            return self.x[idx]
        return self.x[idx],self.y[idx]

    def __len__(self):
        return len(self.x)

trainset = DigitDataset(train_x,train_y)
validset = DigitDataset(valid_x,valid_y)
train_dl = DataLoader(trainset,batch_size = CONFIG.batch_size,shuffle=True)
valid_dl = DataLoader(validset,batch_size = CONFIG.batch_size)

class DigitModel(nn.Module):

    def __init__(self):
        super(DigitModel,self).__init__()

        self.model = nn.Sequential(
            nn.Conv2d(1,32,3),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32,8,3),
            nn.BatchNorm2d(8),
            nn.Flatten(),
            nn.Linear(24*24*8,256),
            nn.ReLU(),
            nn.Linear(256,128),
            nn.Dropout(0.2),
            nn.ReLU(),
            nn.Linear(128,10)
        )

    def forward(self,x):
        x = self.model(x)
        x = torch.softmax(x,dim=-1)
        return x

model = DigitModel().to(device)
lossfn = nn.CrossEntropyLoss()
opt = torch.optim.Adam(model.parameters(), lr=CONFIG.lr)

for e in range(CONFIG.epoch):
    print(f"Training on progress... {e+1}/{CONFIG.epoch}")
    acc =0
    total =0

    model.train() # train
    for x,y in train_dl:
        x = x.to(device)
        y = y.to(device)
        y_hat = model(x)
        loss = lossfn(y_hat,y)

        opt.zero_grad()
        loss.backward()
        opt.step()

    model.eval() #valid
    with torch.no_grad():
        for vx,vy in valid_dl:
            vx = vx.to(device)
            vy = vy.to(device)

            vy_hat = model(vx)
            pred = vy_hat.max(dim=1)[1]
            acc += (pred == vy).sum().item()
    print(f"Epoch {e+1} : Acc {100*acc/3000}")

df_test= pd.read_csv(PATH.TEST)
l = len(df_test)
test_x = torch.Tensor(df_test.values).reshape(l,28,28) / 256
test_x = test_x.unsqueeze(1).to(device)

outputs = model(test_x)
_, pred = torch.max(outputs, 1)
pred = pred.cpu()

for i,img in enumerate(test_x[50:56]) :
    plt.subplot(2,3,i+1)
    plt.axis('off')
    plt.title(f"Predicted : {pred[50+i]}")
    plt.imshow(img.squeeze(0).cpu())

submission = pd.DataFrame({'ImageId': np.arange(1, (pred.size(0) + 1)), 'Label': pred})
submission.to_csv("submission.csv", index = False)